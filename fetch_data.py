import requests
from bs4 import BeautifulSoup
import json
import datetime
import pytz
import re
import os
import sys

# === æ—¥æœ¬æ™‚é–“è¨­å®š ===
JST = pytz.timezone("Asia/Tokyo")
today = datetime.datetime.now(JST).date()

# --- å®Ÿè¡Œã‚ªãƒ—ã‚·ãƒ§ãƒ³ ---
force_program = "--force-program" in sys.argv
force_result = "--force-result" in sys.argv

DATA_FILE = "data.json"

# --- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ---
if os.path.exists(DATA_FILE):
    try:
        with open(DATA_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
    except json.JSONDecodeError:
        print("âš  data.json ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã€‚æ–°è¦ä½œæˆã—ã¾ã™ã€‚")
        data = {}
else:
    data = {}

# === å‡ºèµ°è¡¨å–å¾— ===
def fetch_race_program(date):
    base_url = f"https://www.boatrace.jp/owpc/pc/race/raceindex?hd={date.strftime('%Y%m%d')}"
    print(f"ğŸ” å‡ºèµ°è¡¨å–å¾—: {base_url}")
    res = requests.get(base_url)
    res.encoding = res.apparent_encoding
    soup = BeautifulSoup(res.text, "lxml")

    program = {}

    # å„å ´ãƒªãƒ³ã‚¯ã‚’å–å¾—
    venues = soup.select("ul.tab01_01 li a")
    for v in venues:
        venue_name = v.text.strip()
        href = v.get("href")
        if not href:
            continue
        venue_url = "https://www.boatrace.jp" + href
        print(f"â¡ {venue_name}: {venue_url}")

        try:
            v_res = requests.get(venue_url)
            v_res.encoding = v_res.apparent_encoding
            v_soup = BeautifulSoup(v_res.text, "lxml")

            races = {}
            for race_link in v_soup.select(".race_card_btn"):
                race_no = race_link.text.strip().replace("R", "")
                race_href = race_link.get("href")
                if not race_href:
                    continue
                race_url = "https://www.boatrace.jp" + race_href

                race_data = fetch_race_detail(race_url)
                races[race_no] = race_data

            program[venue_name] = races
        except Exception as e:
            print(f"âš  {venue_name} å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

    return program


# === å„ãƒ¬ãƒ¼ã‚¹è©³ç´° ===
def fetch_race_detail(url):
    res = requests.get(url)
    res.encoding = res.apparent_encoding
    soup = BeautifulSoup(res.text, "lxml")

    race_data = {
        "title": soup.select_one(".heading1_title").text.strip() if soup.select_one(".heading1_title") else "",
        "entries": []
    }

    for row in soup.select(".table1 tbody tr"):
        cols = row.select("td")
        if len(cols) < 8:
            continue

        try:
            grade = cols[1].text.strip()
            name = cols[2].text.strip()
            st = cols[6].text.strip()

            # Fè¡¨ç¤ºå¤‰æ›
            f_flag = "ãƒ¼"
            if "F2" in st:
                f_flag = "F2"
            elif "F1" in st:
                f_flag = "F1"

            # å‹ç‡ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            raw_wr = re.findall(r"\d+\.\d", cols[7].text)
            if len(raw_wr) >= 3:
                national = round(float(raw_wr[0]) * 10, 1)  # å…¨å›½å‹ç‡â†’%
                local = round(float(raw_wr[1]) * 10, 1)     # å½“åœ°å‹ç‡â†’%
                motor = round(float(raw_wr[2]) * 10, 1)     # ãƒ¢ãƒ¼ã‚¿ãƒ¼å‹ç‡â†’%
            else:
                national = local = motor = 0.0

            # AIè©•ä¾¡ (ç°¡æ˜“ãƒ­ã‚¸ãƒƒã‚¯)
            if national >= 70:
                ai_eval = "â—"
            elif national >= 60:
                ai_eval = "â—‹"
            else:
                ai_eval = "â–²"

            race_data["entries"].append({
                "grade": grade,
                "name": name,
                "st": st,
                "f": f_flag,
                "national_rate": f"{national}%",
                "local_rate": f"{local}%",
                "motor_rate": f"{motor}%",
                "ai_eval": ai_eval
            })
        except Exception as e:
            print(f"âš  é¸æ‰‹ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºå¤±æ•—: {e}")

    return race_data


# === çµæœãƒ‡ãƒ¼ã‚¿å–å¾— ===
def fetch_results(date):
    result_url = f"https://www.boatrace.jp/owpc/pc/race/raceresultall?hd={date.strftime('%Y%m%d')}"
    print(f"ğŸ çµæœå–å¾—: {result_url}")
    res = requests.get(result_url)
    res.encoding = res.apparent_encoding
    soup = BeautifulSoup(res.text, "lxml")

    results = {}
    for link in soup.select(".table1 a"):
        href = link.get("href")
        if not href:
            continue
        venue = link.text.strip()
        full_url = "https://www.boatrace.jp" + href
        results[venue] = full_url
    return results


# === å®Ÿè¡Œ ===
if force_program or not data.get(str(today)):
    print("ğŸ“¦ å‡ºèµ°è¡¨ã‚’æ›´æ–°ä¸­...")
    program_data = fetch_race_program(today)
    data[str(today)] = {"program": program_data}
else:
    print("âœ… å‡ºèµ°è¡¨ã¯æœ€æ–°ã§ã™ã€‚")

if force_result:
    print("ğŸ“¦ çµæœã‚’æ›´æ–°ä¸­...")
    result_data = fetch_results(today)
    data[str(today)]["results"] = result_data
else:
    print("âœ… çµæœã¯æœ€æ–°ã§ã™ã€‚")

# === ä¿å­˜ ===
with open(DATA_FILE, "w", encoding="utf-8") as f:
    json.dump(data, f, ensure_ascii=False, indent=2)

print(f"âœ… data.json æ›´æ–°å®Œäº† ({today})")