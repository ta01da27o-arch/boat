import requests
from bs4 import BeautifulSoup
import datetime
import json
from pathlib import Path
import time

# === å®šæ•°è¨­å®š ===
BASE_URL = "https://www.boatrace.jp"
DATA_FILE = Path("data.json")

# === 24å ´ã‚³ãƒ¼ãƒ‰ ===
VENUES = {
    "01": "æ¡ç”Ÿ", "02": "æˆ¸ç”°", "03": "æ±Ÿæˆ¸å·", "04": "å¹³å’Œå³¶", "05": "å¤šæ‘©å·", "06": "æµœåæ¹–",
    "07": "è’²éƒ¡", "08": "å¸¸æ»‘", "09": "æ´¥", "10": "ä¸‰å›½", "11": "ã³ã‚ã“", "12": "ä½ä¹‹æ±Ÿ",
    "13": "å°¼å´", "14": "é³´é–€", "15": "ä¸¸äº€", "16": "å…å³¶", "17": "å®®å³¶", "18": "å¾³å±±",
    "19": "ä¸‹é–¢", "20": "è‹¥æ¾", "21": "èŠ¦å±‹", "22": "ç¦å²¡", "23": "å”æ´¥", "24": "å¤§æ‘"
}

# === ä»Šæ—¥ã®æ—¥ä»˜ ===
today = datetime.date.today()
date_str = today.strftime("%Y%m%d")

# === å…±é€š: HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆãƒªãƒˆãƒ©ã‚¤ï¼‹ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰ ===
def safe_get(url, retries=3, delay=3, timeout=15):
    """é€šä¿¡ã‚¨ãƒ©ãƒ¼æ™‚ã«è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤"""
    for i in range(retries):
        try:
            res = requests.get(url, timeout=timeout)
            res.raise_for_status()
            return res
        except Exception as e:
            print(f"â³ retry {i+1}/{retries}: {e}")
            time.sleep(delay)
    print(f"âŒ æ¥ç¶šå¤±æ•—: {url}")
    return None

# === æœ¬æ—¥é–‹å‚¬å ´ã‚’å–å¾— ===
def get_today_venues():
    url = f"{BASE_URL}/owpc/pc/race/index"
    res = safe_get(url)
    if not res:
        print("âŒ é–‹å‚¬å ´å–å¾—å¤±æ•—")
        return []

    soup = BeautifulSoup(res.text, "lxml")
    links = soup.select("li.is-holding a")
    venue_codes = []
    for link in links:
        href = link.get("href", "")
        if "jcd=" in href:
            code = href.split("jcd=")[1].split("&")[0]
            if code in VENUES:
                venue_codes.append(code)
    if venue_codes:
        print(f"âœ… é–‹å‚¬å ´: {', '.join([VENUES[v] for v in venue_codes])}")
    else:
        print("âš ï¸ é–‹å‚¬å ´ãªã—")
    return venue_codes

# === å„å ´ã®ãƒ¬ãƒ¼ã‚¹ä¸€è¦§ã‚’å–å¾— ===
def fetch_race_program(venue_code):
    program = {"title": VENUES[venue_code], "races": []}

    for rno in range(1, 13):
        url = f"{BASE_URL}/owpc/pc/race/racelist?rno={rno}&jcd={venue_code}&hd={date_str}"
        res = safe_get(url)
        if not res:
            continue

        soup = BeautifulSoup(res.text, "lxml")

        title = soup.select_one(".heading2_title")
        race_name = title.text.strip() if title else f"{rno}R"
        boats = [b.text.strip() for b in soup.select(".table1_boatImage1 .is-fs18")]
        players = [p.text.strip() for p in soup.select(".table1_name")]

        if not boats:
            continue  # ãƒ¬ãƒ¼ã‚¹æœªé–‹å‚¬ã¾ãŸã¯å–å¾—ä¸å¯

        race_info = {
            "no": rno,
            "name": race_name,
            "boats": boats,
            "players": players,
        }
        program["races"].append(race_info)

    print(f"ğŸ“¦ {VENUES[venue_code]}: {len(program['races'])}R å–å¾—å®Œäº†")
    return program

# === ãƒ¡ã‚¤ãƒ³ ===
def main():
    all_data = {"date": date_str, "program": {}, "results": {}}

    venues = get_today_venues()
    if not venues:
        print("âš ï¸ é–‹å‚¬å ´ãªã— - ç©ºãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜")
        with open(DATA_FILE, "w", encoding="utf-8") as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        print("âœ… data.json ã‚’æ›´æ–°ã—ã¾ã—ãŸ (0å ´)")
        return

    for v in venues:
        program = fetch_race_program(v)
        all_data["program"][v] = program

    # JSONä¿å­˜
    with open(DATA_FILE, "w", encoding="utf-8") as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… data.json ã‚’æ›´æ–°ã—ã¾ã—ãŸ ({len(venues)}å ´)")

# === å®Ÿè¡Œ ===
if __name__ == "__main__":
    main()