# fetch_data.py
import requests
from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning
import json
from datetime import datetime
import time
import warnings

warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)

BASE_URL = "https://www.boatrace.jp"
INDEX_URL = f"{BASE_URL}/owpc/pc/race/index"
OUTPUT_FILE = "data.json"

# ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¨ãƒªãƒˆãƒ©ã‚¤è¨­å®š
TIMEOUT = 15
RETRY_COUNT = 3

def fetch_url(url):
    """HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰"""
    for i in range(RETRY_COUNT):
        try:
            res = requests.get(url, timeout=TIMEOUT)
            res.raise_for_status()
            return res
        except requests.exceptions.RequestException as e:
            print(f"â³ retry {i+1}/{RETRY_COUNT}: {e}")
            time.sleep(2)
    return None

def get_today_races():
    """æœ¬æ—¥ã®å…¨é–‹å‚¬å ´ã‚’å–å¾—"""
    print("ğŸ æœ¬æ—¥ã®é–‹å‚¬å ´ä¸€è¦§ã‚’å–å¾—ä¸­...")
    res = fetch_url(INDEX_URL)
    if not res:
        print("âŒ é–‹å‚¬å ´ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•—")
        return []

    soup = BeautifulSoup(res.text, "lxml")

    links = soup.select("a[href*='/owpc/pc/race/racelist']")
    venues = []

    for a in links:
        href = a.get("href")
        if href and "/owpc/pc/race/racelist" in href:
            venue_url = BASE_URL + href
            venue_name = a.text.strip()
            venues.append({"name": venue_name, "url": venue_url})

    if not venues:
        print("âš ï¸ é–‹å‚¬å ´ãªã—")
    else:
        print(f"âœ… {len(venues)}å ´ã‚’æ¤œå‡º")

    return venues

def get_race_details(venue):
    """å„é–‹å‚¬å ´ã®ãƒ¬ãƒ¼ã‚¹è©³ç´°ã‚’å–å¾—"""
    res = fetch_url(venue["url"])
    if not res:
        print(f"âŒ {venue['name']} ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—")
        return []

    soup = BeautifulSoup(res.text, "lxml")
    race_links = soup.select("a[href*='/owpc/pc/race/racedata']")
    races = []

    for a in race_links:
        href = a.get("href")
        if href:
            race_url = BASE_URL + href
            race_name = a.text.strip()
            races.append({"race": race_name, "url": race_url})

    print(f"ğŸ“¦ {venue['name']} - {len(races)}ãƒ¬ãƒ¼ã‚¹å–å¾—")
    return races

def main():
    print("ğŸš€ æœ¬æ—¥ã®ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹")
    today = datetime.now().strftime("%Y/%m/%d")

    venues = get_today_races()
    if not venues:
        print("âš ï¸ é–‹å‚¬å ´ãªã— - ç©ºãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜")
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            json.dump({"date": today, "venues": []}, f, ensure_ascii=False, indent=2)
        print(f"âœ… {OUTPUT_FILE} ã‚’æ›´æ–°ã—ã¾ã—ãŸ (0å ´)")
        return

    all_data = {"date": today, "venues": []}

    for v in venues:
        venue_races = get_race_details(v)
        all_data["venues"].append({
            "name": v["name"],
            "url": v["url"],
            "races": venue_races
        })

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… {OUTPUT_FILE} ã‚’æ›´æ–°ã—ã¾ã—ãŸ ({len(venues)}å ´)")
    print("ğŸ¯ æœ¬æ—¥ã®å…¨ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†ï¼")

if __name__ == "__main__":
    main()